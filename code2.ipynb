{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 21:00:50.302507: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-10 21:00:51.962604: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-10 21:00:52.723530: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-10 21:00:57.828649: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:\n",
      "2025-02-10 21:00:57.830868: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:\n",
      "2025-02-10 21:00:57.830897: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 21:01:09.254054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-10 21:01:09.512534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-10 21:01:09.512910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "!source /etc/profile.d/modules.sh\n",
    "!module load CUDA/11.2\n",
    "!export PATH=/local/java/cuda-11.2/bin:$PATH\n",
    "!export LD_LIBRARY_PATH=/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:$LD_LIBRARY_PATH  # this line is needed for it to recognise gpu devices -- run this in the terminal\n",
    "!export CUDA_HOME=/local/java/cuda-11.2\n",
    "import os \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten, Dropout, GlobalAveragePooling2D, Lambda\n",
    "from datetime import datetime\n",
    "from keras import backend as K\n",
    "\n",
    "print(tf.__version__)  # 2.10.0\n",
    "print(tf.config.list_physical_devices('GPU'))  # should show gpu available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 21:01:09.565478: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-10 21:01:09.566894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-10 21:01:09.567149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-10 21:01:09.567281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-10 21:01:12.173292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-10 21:01:12.176238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-10 21:01:12.176408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-10 21:01:12.176554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9478 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "def preprocess(image_path):\n",
    "    # read the file \n",
    "    raw = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_image(raw, expand_animations=False, channels = 3)\n",
    "    img = tf.image.resize(img, size = (224, 224), preserve_aspect_ratio=True)\n",
    "    img = tf.image.resize_with_crop_or_pad(img, 224, 224)\n",
    "    img = tf.cast(img, tf.float32)/255.0\n",
    "    return img \n",
    "\n",
    "def preprocess_pair(pair):\n",
    "    imgA = preprocess(pair[0])\n",
    "    imgB = preprocess(pair[1])\n",
    "    return (imgA, imgB)\n",
    "\n",
    "class RandomInvert(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_value = 255, factor=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.factor = factor\n",
    "        self.max_value = max_value\n",
    "\n",
    "    def call(self, x):\n",
    "        if  tf.random.uniform([]) < self.factor:\n",
    "            x = (self.max_value - x)\n",
    "        return x\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    RandomInvert(max_value = 1.0),\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomRotation((-0.4, 0.4)),\n",
    "    tf.keras.layers.RandomBrightness(factor=(-0.2, 0.2), value_range=(0., 1.)),\n",
    "    tf.keras.layers.GaussianNoise(0.005),\n",
    "    tf.keras.layers.RandomZoom(height_factor=(-0.4, 0.4)),\n",
    "    tf.keras.layers.RandomContrast(factor=(0.1, 0.9)),\n",
    "    tf.keras.layers.RandomTranslation(height_factor=0.2, width_factor=0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (224, 224, 3)\n",
    "BATCH_SIZE_TRAIN = 8\n",
    "BATCH_SIZE_VALIDATION = 2\n",
    "MARGIN = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 96 images.\n",
      "[0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 4 4\n",
      " 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n",
      "['train_separated/pencil_case/20250204_193242.jpg', 'train_separated/pencil_case/20250204_203128.jpg', 'train_separated/pencil_case/20250204_193301.jpg', 'train_separated/pencil_case/20250204_202623.jpg', 'train_separated/pencil_case/LL83529-PN_Lihit-Lab-Lying-Pen-Pouch-PuniLabo-Penguin_P3.jpg', 'train_separated/pencil_case/200623.jpg', 'train_separated/pencil_case/200628.jpg', 'train_separated/pencil_case/20250204_193312.jpg', 'train_separated/camera/sony_zv_e10_06.jpg', 'train_separated/camera/20250204_204308.jpg', 'train_separated/camera/sony_zve10_kit_1.jpg', 'train_separated/camera/20250204_203455.jpg', 'train_separated/camera/20250204_203503.jpg', 'train_separated/camera/zve10kittop_500x.jpg', 'train_separated/camera/design-medium.jpg', 'train_separated/camera/20250204_203501.jpg', 'train_separated/camera/20250204_194507.jpg', 'train_separated/digital_watch/51L5xGWtlnL._AC_UF1000,1000_QL80_.jpg', 'train_separated/digital_watch/20250204_190310.jpg', 'train_separated/digital_watch/20250204_190254.jpg', 'train_separated/digital_watch/20250204_190357.jpg', 'train_separated/digital_watch/20250204_204424.jpg', 'train_separated/digital_watch/20250204_190417.jpg', 'train_separated/digital_watch/20250204_203149.jpg', 'train_separated/digital_watch/20250204_202410.jpg', 'train_separated/digital_watch/samsung-galaxy-watch-6-40mm-black-1-800x800.jpg', 'train_separated/digital_watch/20250204_190303.jpg', 'train_separated/wallet/20250131_001405.jpg', 'train_separated/wallet/20250204_203202.jpg', 'train_separated/wallet/sg-11134201-7rbki-lqczx080rksn43.jpeg', 'train_separated/wallet/20250131_001416.jpg', 'train_separated/wallet/20250204_204354.jpg', 'train_separated/wallet/20250131_001358.jpg', 'train_separated/wallet/20250131_001352.jpg', 'train_separated/wallet/20250131_001408.jpg', 'train_separated/mouse/20250204_192929.jpg', 'train_separated/mouse/20250204_204531.jpg', 'train_separated/mouse/20250204_202623.jpg', 'train_separated/mouse/20250204_202503.jpg', 'train_separated/mouse/61DaQf+poJL._AC_UF1000,1000_QL80_.jpg', 'train_separated/mouse/20250204_192535.jpg', 'train_separated/mouse/images.jpeg', 'train_separated/mouse/m-xgm10db_z1.jpg', 'train_separated/water_bottle/20250204_165751.jpg', 'train_separated/water_bottle/s-l400.jpg', 'train_separated/water_bottle/20250204_170044.jpg', 'train_separated/water_bottle/20250204_202922.jpg', 'train_separated/water_bottle/20250204_201246.jpg', 'train_separated/water_bottle/20250204_170049.jpg', 'train_separated/water_bottle/20250204_170104.jpg', 'train_separated/water_bottle/20250204_203419.jpg', 'train_separated/water_bottle/31katMtAaZL.jpg', 'train_separated/backpack/3812321-1348-Giga_marine_ink-D-05.png', 'train_separated/backpack/41SIyEXoYSL._AC_.jpg', 'train_separated/backpack/20250204_174530.jpg', 'train_separated/backpack/20250204_204624.jpg', 'train_separated/backpack/DEUT01538_01.jpg', 'train_separated/backpack/20250204_201235.jpg', 'train_separated/backpack/20250204_203345.jpg', 'train_separated/backpack/20250204_202652.jpg', 'train_separated/alarm_clock/20250209_202847.jpg', 'train_separated/alarm_clock/20250209_202808.jpg', 'train_separated/alarm_clock/20250209_202200.jpg', 'train_separated/alarm_clock/20250209_202634.jpg', 'train_separated/alarm_clock/71h-2SPFJlL.jpg', 'train_separated/alarm_clock/20250204_171523.jpg', 'train_separated/alarm_clock/20250209_202815.jpg', 'train_separated/alarm_clock/20250204_210702.jpg', 'train_separated/alarm_clock/20250209_202239.jpg', 'train_separated/alarm_clock/20250209_202308.jpg', 'train_separated/alarm_clock/20250204_173420.jpg', 'train_separated/alarm_clock/1200x1200.jpg', 'train_separated/alarm_clock/20250204_203139.jpg', 'train_separated/alarm_clock/20250209_202156.jpg', 'train_separated/alarm_clock/20250209_202629.jpg', 'train_separated/alarm_clock/20250209_202707.jpg', 'train_separated/alarm_clock/20250209_202251.jpg', 'train_separated/alarm_clock/shopping.jpg', 'train_separated/alarm_clock/20250204_171515.jpg', 'train_separated/alarm_clock/20250204_173437.jpg', 'train_separated/alarm_clock/20250209_201749.jpg', 'train_separated/alarm_clock/20250209_202824.jpg', 'train_separated/alarm_clock/20250209_202232.jpg', 'train_separated/alarm_clock/20250209_202258.jpg', 'train_separated/alarm_clock/20250209_202756.jpg', 'train_separated/alarm_clock/20250204_204340.jpg', 'train_separated/alarm_clock/20250204_201617.jpg', 'train_separated/alarm_clock/20250209_201758.jpg', 'train_separated/alarm_clock/20250209_201741.jpg', 'train_separated/alarm_clock/20250209_202700.jpg', 'train_separated/alarm_clock/20250209_202642.jpg', 'train_separated/alarm_clock/20250209_201734.jpg', 'train_separated/alarm_clock/20250209_202732.jpg', 'train_separated/alarm_clock/20250209_202747.jpg', 'train_separated/alarm_clock/20250204_173408.jpg', 'train_separated/alarm_clock/20250209_201744.jpg']\n",
      "['pencil_case' 'camera' 'digital_watch' 'wallet' 'mouse' 'water_bottle'\n",
      " 'backpack' 'alarm_clock']\n",
      "8 classes:  ['pencil_case' 'camera' 'digital_watch' 'wallet' 'mouse' 'water_bottle'\n",
      " 'backpack' 'alarm_clock']\n",
      "74\n",
      "74\n",
      "22\n",
      "22\n",
      "['train_separated/pencil_case/20250204_193242.jpg', 'train_separated/pencil_case/20250204_203128.jpg', 'train_separated/pencil_case/20250204_193301.jpg', 'train_separated/pencil_case/20250204_202623.jpg']\n",
      "[0, 0, 0, 0]\n",
      "['train_separated/pencil_case/200628.jpg', 'train_separated/pencil_case/20250204_193312.jpg', 'train_separated/camera/20250204_203501.jpg', 'train_separated/camera/20250204_194507.jpg']\n",
      "[0, 0, 1, 1]\n",
      "74\n",
      "22\n",
      "80\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"train_separated\"\n",
    "\n",
    "files = [os.path.join(r,file) for r,d,f in os.walk(dataset_path) for file in f]\n",
    "file_size = len(files)\n",
    "print(\"We have \" + str(file_size) + \" images.\")\n",
    "# random.shuffle(files)\n",
    "\n",
    "def make_instances(files, classes = []):\n",
    "    labels = []\n",
    "    for i in range(len(files)):\n",
    "        file = files[i]\n",
    "        clazz = file.split(\"/\")[-2]\n",
    "        \n",
    "        if classes.count(clazz):\n",
    "            label = classes.index(clazz)\n",
    "        else:\n",
    "            label = len(classes)\n",
    "            classes.append(clazz)\n",
    "\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(labels), np.array(classes)\n",
    "\n",
    "labels, classes = make_instances(files)\n",
    "print(labels)\n",
    "print(files)\n",
    "print(classes)\n",
    "CLASSES_SIZE = len(classes)\n",
    "\n",
    "print(str(CLASSES_SIZE) + \" classes: \", classes)\n",
    "\n",
    "training_files = []\n",
    "training_labels = []\n",
    "validation_files = []\n",
    "validation_labels = []\n",
    "\n",
    "for label in range(CLASSES_SIZE):\n",
    "    indexes = np.where(labels == label)[0]\n",
    "\n",
    "    threshold = len(indexes) * 80 // 100\n",
    "\n",
    "    training_indexes = indexes[0:threshold]\n",
    "    training_files_for_class = [files[i] for i in training_indexes]\n",
    "\n",
    "    training_files.extend(training_files_for_class)\n",
    "    training_labels.extend([label] * len(training_files_for_class))\n",
    "\n",
    "    validation_indexes = indexes[threshold:]\n",
    "    validation_files_for_class = [files[i] for i in validation_indexes]\n",
    "\n",
    "    validation_files.extend(validation_files_for_class)\n",
    "    validation_labels.extend([label] * len(validation_files_for_class))\n",
    "\n",
    "print(len(training_files))\n",
    "print(len(training_labels))\n",
    "print(len(validation_files))\n",
    "print(len(validation_labels))\n",
    "\n",
    "print(training_files[0:4])\n",
    "print(training_labels[0:4])\n",
    "\n",
    "print(validation_files[0:4])\n",
    "print(validation_labels[0:4])\n",
    "\n",
    "def create_pairs(x, digit_indices):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "\n",
    "    n = min([len(digit_indices[d]) for d in range(CLASSES_SIZE)]) - 1\n",
    "    \n",
    "    for d in range(CLASSES_SIZE):\n",
    "        for i in range(n):\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            inc = random.randrange(1, CLASSES_SIZE)\n",
    "            dn = (d + inc) % CLASSES_SIZE\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            labels += [1, 0]\n",
    "            \n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "\n",
    "def create_pairs_on_set(images, labels):\n",
    "    \n",
    "    digit_indices = [np.where(labels == i)[0] for i in range(CLASSES_SIZE)]\n",
    "    pairs, y = create_pairs(images, digit_indices)\n",
    "    y = y.astype('float32')\n",
    "    \n",
    "    return pairs, y\n",
    "\n",
    "training_files = np.array(training_files)\n",
    "training_labels = np.array(training_labels)\n",
    "\n",
    "validation_files = np.array(validation_files)\n",
    "validation_labels = np.array(validation_labels)\n",
    "\n",
    "training_pairs, training_pairs_labels = create_pairs_on_set(training_files, training_labels)\n",
    "validation_pairs, validation_pairs_labels = create_pairs_on_set(validation_files, validation_labels)\n",
    "\n",
    "print(len(training_files))\n",
    "print(len(validation_files))\n",
    "print(len(training_pairs))\n",
    "print(len(validation_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "def build_training_dataset():\n",
    "\n",
    "    pairs_tensor = tf.convert_to_tensor(training_pairs)\n",
    "    labels_tensor = tf.convert_to_tensor(training_pairs_labels)\n",
    "\n",
    "    result = tf.data.Dataset.from_tensor_slices((pairs_tensor, labels_tensor))\n",
    "\n",
    "    result = result.map(lambda pair, label: (preprocess_pair(pair), label))\n",
    "    result = result.shuffle(128, reshuffle_each_iteration=True)\n",
    "    result = result.repeat()\n",
    "    result = result.batch(BATCH_SIZE_TRAIN)\n",
    "    result = result.map(lambda pair, y: ((data_augmentation(pair[0], training=True),data_augmentation(pair[1], training=True)), y), \n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    result = result.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return result\n",
    "\n",
    "train_dataset = build_training_dataset()\n",
    "\n",
    "def build_validation_dataset():\n",
    "\n",
    "    pairs_tensor = tf.convert_to_tensor(validation_pairs)\n",
    "    labels_tensor = tf.convert_to_tensor(validation_pairs_labels)\n",
    "\n",
    "    result = tf.data.Dataset.from_tensor_slices((pairs_tensor, labels_tensor))\n",
    "\n",
    "    result = result.map(lambda pair, label: (preprocess_pair(pair), label))\n",
    "    result = result.batch(BATCH_SIZE_VALIDATION)\n",
    "    result = result.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return result\n",
    "\n",
    "validation_dataset = build_validation_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vectors):\n",
    "    x, y = vectors\n",
    "    sum_squared = K.sum(K.square(x-y), axis = 1, keepdims= True)\n",
    "    return K.sqrt(K.maximum(sum_squared, K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make embedding\n",
    "embedding is each sub, identical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding():\n",
    "    inputs = tf.keras.layers.Input(INPUT_SHAPE)\n",
    "    base_model = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=INPUT_SHAPE, include_top=False, weights='imagenet')\n",
    "    \n",
    "    base_model.trainable = True\n",
    "    limit = len(base_model.layers)-int(len(base_model.layers)*.10)\n",
    "    for layer in base_model.layers[:limit]:\n",
    "        layer.trainable =  False\n",
    "          \n",
    "    x = base_model(inputs)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs=tf.keras.layers.Dense(64)(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model():\n",
    "    # create embedding\n",
    "    embedding = make_embedding()\n",
    "\n",
    "    # create the same embedding for the two inputs \n",
    "    input_a = Input(shape = INPUT_SHAPE, name = \"first_image\")\n",
    "    input_b = Input(shape = INPUT_SHAPE, name = \"second_image\")\n",
    "\n",
    "    embedding_a = embedding(input_a)\n",
    "    embedding_b = embedding(input_b)\n",
    "\n",
    "    # Create the final euclidean distance layer\n",
    "    output = Lambda(euclidean_distance, name = \"distance\")([embedding_a, embedding_b])\n",
    "\n",
    "    return Model([input_a, input_b], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " first_image (InputLayer)       [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " second_image (InputLayer)      [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 64)           2339968     ['first_image[0][0]',            \n",
      "                                                                  'second_image[0][0]']           \n",
      "                                                                                                  \n",
      " distance (Lambda)              (None, 1)            0           ['model[0][0]',                  \n",
      "                                                                  'model[1][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,339,968\n",
      "Trainable params: 1,121,984\n",
      "Non-trainable params: 1,217,984\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_model = make_siamese_model()\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(MARGIN - y_pred, 0))\n",
    "    return (y_true * square_pred + (1 - y_true) * margin_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Precision(tf.keras.metrics.Precision):\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_fix = tf.math.less(y_pred, 0.5) \n",
    "        y_pred_fix = tf.cast(y_pred_fix, y_pred.dtype)\n",
    "       \n",
    "        return super().update_state(y_true, y_pred_fix, sample_weight)\n",
    "  \n",
    "class Custom_Recall(tf.keras.metrics.Recall):\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_fix = tf.math.less(y_pred, 0.5) \n",
    "        y_pred_fix = tf.cast(y_pred_fix, y_pred.dtype)\n",
    "       \n",
    "        return super().update_state(y_true, y_pred_fix, sample_weight)\n",
    "  \n",
    "class Custom_Accuracy(tf.keras.metrics.Accuracy):\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_fix = tf.math.less(y_pred, 0.5) \n",
    "        y_pred_fix = tf.cast(y_pred_fix, y_pred.dtype)\n",
    "       \n",
    "        return super().update_state(y_true, y_pred_fix, sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 21:01:27.025795: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2025-02-10 21:01:32.053759: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-02-10 21:01:32.054918: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-02-10 21:01:32.054945: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2025-02-10 21:01:32.056048: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-02-10 21:01:32.056133: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2025-02-10 21:01:35.091215: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "789/791 [============================>.] - ETA: 0s - loss: 0.4181 - accuracy: 0.5610 - custom__precision: 0.5548 - custom__recall: 0.6179\n",
      "Epoch 1: val_loss improved from inf to 0.23936, saving model to weights/best_fit.hdf5\n",
      "791/791 [==============================] - 41s 34ms/step - loss: 0.4177 - accuracy: 0.5612 - custom__precision: 0.5548 - custom__recall: 0.6175 - val_loss: 0.2394 - val_accuracy: 0.6520 - val_custom__precision: 0.7201 - val_custom__recall: 0.4972\n",
      "Epoch 2/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.2485 - accuracy: 0.6021 - custom__precision: 0.5909 - custom__recall: 0.6640\n",
      "Epoch 2: val_loss improved from 0.23936 to 0.22381, saving model to weights/best_fit.hdf5\n",
      "791/791 [==============================] - 25s 32ms/step - loss: 0.2485 - accuracy: 0.6022 - custom__precision: 0.5913 - custom__recall: 0.6639 - val_loss: 0.2238 - val_accuracy: 0.6597 - val_custom__precision: 0.7227 - val_custom__recall: 0.5182\n",
      "Epoch 3/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.2262 - accuracy: 0.6373 - custom__precision: 0.6185 - custom__recall: 0.7150\n",
      "Epoch 3: val_loss did not improve from 0.22381\n",
      "791/791 [==============================] - 24s 31ms/step - loss: 0.2262 - accuracy: 0.6373 - custom__precision: 0.6185 - custom__recall: 0.7150 - val_loss: 0.2563 - val_accuracy: 0.6225 - val_custom__precision: 0.7796 - val_custom__recall: 0.3417\n",
      "Epoch 4/200\n",
      "789/791 [============================>.] - ETA: 0s - loss: 0.2145 - accuracy: 0.6624 - custom__precision: 0.6387 - custom__recall: 0.7489\n",
      "Epoch 4: val_loss did not improve from 0.22381\n",
      "791/791 [==============================] - 24s 31ms/step - loss: 0.2145 - accuracy: 0.6623 - custom__precision: 0.6386 - custom__recall: 0.7493 - val_loss: 0.2332 - val_accuracy: 0.6450 - val_custom__precision: 0.7775 - val_custom__recall: 0.4062\n",
      "Epoch 5/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.2048 - accuracy: 0.6866 - custom__precision: 0.6632 - custom__recall: 0.7585\n",
      "Epoch 5: val_loss did not improve from 0.22381\n",
      "791/791 [==============================] - 24s 30ms/step - loss: 0.2048 - accuracy: 0.6866 - custom__precision: 0.6632 - custom__recall: 0.7585 - val_loss: 0.2406 - val_accuracy: 0.6562 - val_custom__precision: 0.8213 - val_custom__recall: 0.3992\n",
      "Epoch 6/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.1948 - accuracy: 0.7035 - custom__precision: 0.6812 - custom__recall: 0.7644\n",
      "Epoch 6: val_loss improved from 0.22381 to 0.21056, saving model to weights/best_fit.hdf5\n",
      "791/791 [==============================] - 23s 30ms/step - loss: 0.1948 - accuracy: 0.7035 - custom__precision: 0.6812 - custom__recall: 0.7644 - val_loss: 0.2106 - val_accuracy: 0.6940 - val_custom__precision: 0.8579 - val_custom__recall: 0.4650\n",
      "Epoch 7/200\n",
      "789/791 [============================>.] - ETA: 0s - loss: 0.1874 - accuracy: 0.7193 - custom__precision: 0.6966 - custom__recall: 0.7765\n",
      "Epoch 7: val_loss improved from 0.21056 to 0.20191, saving model to weights/best_fit.hdf5\n",
      "791/791 [==============================] - 22s 27ms/step - loss: 0.1873 - accuracy: 0.7192 - custom__precision: 0.6964 - custom__recall: 0.7768 - val_loss: 0.2019 - val_accuracy: 0.7045 - val_custom__precision: 0.8303 - val_custom__recall: 0.5140\n",
      "Epoch 8/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.1819 - accuracy: 0.7312 - custom__precision: 0.7126 - custom__recall: 0.7755\n",
      "Epoch 8: val_loss improved from 0.20191 to 0.17714, saving model to weights/best_fit.hdf5\n",
      "791/791 [==============================] - 22s 28ms/step - loss: 0.1820 - accuracy: 0.7309 - custom__precision: 0.7122 - custom__recall: 0.7754 - val_loss: 0.1771 - val_accuracy: 0.7374 - val_custom__precision: 0.8452 - val_custom__recall: 0.5812\n",
      "Epoch 9/200\n",
      "789/791 [============================>.] - ETA: 0s - loss: 0.1801 - accuracy: 0.7285 - custom__precision: 0.7054 - custom__recall: 0.7845\n",
      "Epoch 9: val_loss did not improve from 0.17714\n",
      "791/791 [==============================] - 22s 28ms/step - loss: 0.1801 - accuracy: 0.7288 - custom__precision: 0.7058 - custom__recall: 0.7844 - val_loss: 0.1824 - val_accuracy: 0.7367 - val_custom__precision: 0.8177 - val_custom__recall: 0.6092\n",
      "Epoch 10/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.1755 - accuracy: 0.7441 - custom__precision: 0.7237 - custom__recall: 0.7915\n",
      "Epoch 10: val_loss did not improve from 0.17714\n",
      "791/791 [==============================] - 22s 27ms/step - loss: 0.1756 - accuracy: 0.7442 - custom__precision: 0.7238 - custom__recall: 0.7912 - val_loss: 0.1854 - val_accuracy: 0.7290 - val_custom__precision: 0.8399 - val_custom__recall: 0.5658\n",
      "Epoch 11/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.1700 - accuracy: 0.7585 - custom__precision: 0.7411 - custom__recall: 0.7933\n",
      "Epoch 11: val_loss did not improve from 0.17714\n",
      "791/791 [==============================] - 22s 27ms/step - loss: 0.1700 - accuracy: 0.7585 - custom__precision: 0.7411 - custom__recall: 0.7936 - val_loss: 0.2811 - val_accuracy: 0.6681 - val_custom__precision: 0.8593 - val_custom__recall: 0.4020\n",
      "Epoch 12/200\n",
      "789/791 [============================>.] - ETA: 0s - loss: 0.1674 - accuracy: 0.7563 - custom__precision: 0.7354 - custom__recall: 0.8006\n",
      "Epoch 12: val_loss improved from 0.17714 to 0.16957, saving model to weights/best_fit.hdf5\n",
      "791/791 [==============================] - 22s 28ms/step - loss: 0.1675 - accuracy: 0.7560 - custom__precision: 0.7350 - custom__recall: 0.8001 - val_loss: 0.1696 - val_accuracy: 0.7633 - val_custom__precision: 0.8381 - val_custom__recall: 0.6527\n",
      "Epoch 13/200\n",
      "789/791 [============================>.] - ETA: 0s - loss: 0.1606 - accuracy: 0.7715 - custom__precision: 0.7557 - custom__recall: 0.8030\n",
      "Epoch 13: val_loss improved from 0.16957 to 0.15150, saving model to weights/best_fit.hdf5\n",
      "791/791 [==============================] - 22s 28ms/step - loss: 0.1606 - accuracy: 0.7715 - custom__precision: 0.7552 - custom__recall: 0.8034 - val_loss: 0.1515 - val_accuracy: 0.7843 - val_custom__precision: 0.8152 - val_custom__recall: 0.7353\n",
      "Epoch 14/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.1578 - accuracy: 0.7721 - custom__precision: 0.7491 - custom__recall: 0.8196\n",
      "Epoch 14: val_loss did not improve from 0.15150\n",
      "791/791 [==============================] - 22s 28ms/step - loss: 0.1578 - accuracy: 0.7721 - custom__precision: 0.7491 - custom__recall: 0.8196 - val_loss: 0.1710 - val_accuracy: 0.7647 - val_custom__precision: 0.8351 - val_custom__recall: 0.6597\n",
      "Epoch 15/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.1547 - accuracy: 0.7843 - custom__precision: 0.7689 - custom__recall: 0.8130\n",
      "Epoch 15: val_loss did not improve from 0.15150\n",
      "791/791 [==============================] - 22s 28ms/step - loss: 0.1546 - accuracy: 0.7843 - custom__precision: 0.7690 - custom__recall: 0.8130 - val_loss: 0.2015 - val_accuracy: 0.7269 - val_custom__precision: 0.8750 - val_custom__recall: 0.5294\n",
      "Epoch 16/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.1556 - accuracy: 0.7824 - custom__precision: 0.7624 - custom__recall: 0.8183\n",
      "Epoch 16: val_loss did not improve from 0.15150\n",
      "791/791 [==============================] - 22s 28ms/step - loss: 0.1554 - accuracy: 0.7827 - custom__precision: 0.7626 - custom__recall: 0.8185 - val_loss: 1.2554 - val_accuracy: 0.5980 - val_custom__precision: 0.8465 - val_custom__recall: 0.2395\n",
      "Epoch 17/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.1497 - accuracy: 0.7930 - custom__precision: 0.7789 - custom__recall: 0.8193\n",
      "Epoch 17: val_loss improved from 0.15150 to 0.14678, saving model to weights/best_fit.hdf5\n",
      "791/791 [==============================] - 23s 29ms/step - loss: 0.1497 - accuracy: 0.7930 - custom__precision: 0.7788 - custom__recall: 0.8191 - val_loss: 0.1468 - val_accuracy: 0.8018 - val_custom__precision: 0.8470 - val_custom__recall: 0.7367\n",
      "Epoch 18/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.1539 - accuracy: 0.7799 - custom__precision: 0.7627 - custom__recall: 0.8149\n",
      "Epoch 18: val_loss improved from 0.14678 to 0.13806, saving model to weights/best_fit.hdf5\n",
      "791/791 [==============================] - 23s 29ms/step - loss: 0.1539 - accuracy: 0.7799 - custom__precision: 0.7627 - custom__recall: 0.8149 - val_loss: 0.1381 - val_accuracy: 0.8179 - val_custom__precision: 0.8271 - val_custom__recall: 0.8039\n",
      "Epoch 19/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.1499 - accuracy: 0.7931 - custom__precision: 0.7767 - custom__recall: 0.8204\n",
      "Epoch 19: val_loss did not improve from 0.13806\n",
      "791/791 [==============================] - 23s 29ms/step - loss: 0.1499 - accuracy: 0.7931 - custom__precision: 0.7767 - custom__recall: 0.8204 - val_loss: 0.1520 - val_accuracy: 0.7871 - val_custom__precision: 0.8451 - val_custom__recall: 0.7031\n",
      "Epoch 20/200\n",
      "789/791 [============================>.] - ETA: 0s - loss: 0.1451 - accuracy: 0.8043 - custom__precision: 0.7877 - custom__recall: 0.8348\n",
      "Epoch 20: val_loss did not improve from 0.13806\n",
      "791/791 [==============================] - 23s 29ms/step - loss: 0.1451 - accuracy: 0.8044 - custom__precision: 0.7882 - custom__recall: 0.8343 - val_loss: 0.1418 - val_accuracy: 0.7983 - val_custom__precision: 0.8392 - val_custom__recall: 0.7381\n",
      "Epoch 21/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.1437 - accuracy: 0.8108 - custom__precision: 0.7988 - custom__recall: 0.8293\n",
      "Epoch 21: val_loss improved from 0.13806 to 0.13161, saving model to weights/best_fit.hdf5\n",
      "791/791 [==============================] - 23s 30ms/step - loss: 0.1438 - accuracy: 0.8105 - custom__precision: 0.7985 - custom__recall: 0.8292 - val_loss: 0.1316 - val_accuracy: 0.8284 - val_custom__precision: 0.7928 - val_custom__recall: 0.8894\n",
      "Epoch 22/200\n",
      "789/791 [============================>.] - ETA: 0s - loss: 0.1402 - accuracy: 0.8146 - custom__precision: 0.8027 - custom__recall: 0.8357\n",
      "Epoch 22: val_loss did not improve from 0.13161\n",
      "791/791 [==============================] - 23s 29ms/step - loss: 0.1405 - accuracy: 0.8140 - custom__precision: 0.8022 - custom__recall: 0.8353 - val_loss: 0.1522 - val_accuracy: 0.7801 - val_custom__precision: 0.8497 - val_custom__recall: 0.6807\n",
      "Epoch 23/200\n",
      "789/791 [============================>.] - ETA: 0s - loss: 0.1421 - accuracy: 0.8107 - custom__precision: 0.7940 - custom__recall: 0.8377\n",
      "Epoch 23: val_loss did not improve from 0.13161\n",
      "791/791 [==============================] - 22s 28ms/step - loss: 0.1419 - accuracy: 0.8110 - custom__precision: 0.7944 - custom__recall: 0.8377 - val_loss: 0.1361 - val_accuracy: 0.8095 - val_custom__precision: 0.8212 - val_custom__recall: 0.7913\n",
      "Epoch 24/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.8086 - custom__precision: 0.7994 - custom__recall: 0.8240\n",
      "Epoch 24: val_loss did not improve from 0.13161\n",
      "791/791 [==============================] - 23s 29ms/step - loss: 0.1398 - accuracy: 0.8086 - custom__precision: 0.7994 - custom__recall: 0.8240 - val_loss: 0.1424 - val_accuracy: 0.8074 - val_custom__precision: 0.8652 - val_custom__recall: 0.7283\n",
      "Epoch 25/200\n",
      "789/791 [============================>.] - ETA: 0s - loss: 0.1406 - accuracy: 0.8218 - custom__precision: 0.8101 - custom__recall: 0.8397\n",
      "Epoch 25: val_loss did not improve from 0.13161\n",
      "791/791 [==============================] - 23s 29ms/step - loss: 0.1408 - accuracy: 0.8210 - custom__precision: 0.8096 - custom__recall: 0.8386 - val_loss: 0.1365 - val_accuracy: 0.8067 - val_custom__precision: 0.8349 - val_custom__recall: 0.7647\n",
      "Epoch 26/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.1373 - accuracy: 0.8178 - custom__precision: 0.8017 - custom__recall: 0.8459\n",
      "Epoch 26: val_loss did not improve from 0.13161\n",
      "791/791 [==============================] - 22s 28ms/step - loss: 0.1373 - accuracy: 0.8178 - custom__precision: 0.8017 - custom__recall: 0.8459 - val_loss: 0.1332 - val_accuracy: 0.8130 - val_custom__precision: 0.8465 - val_custom__recall: 0.7647\n",
      "Epoch 27/200\n",
      "789/791 [============================>.] - ETA: 0s - loss: 0.1342 - accuracy: 0.8221 - custom__precision: 0.8147 - custom__recall: 0.8335\n",
      "Epoch 27: val_loss did not improve from 0.13161\n",
      "791/791 [==============================] - 22s 28ms/step - loss: 0.1343 - accuracy: 0.8221 - custom__precision: 0.8145 - custom__recall: 0.8336 - val_loss: 0.1469 - val_accuracy: 0.7808 - val_custom__precision: 0.8679 - val_custom__recall: 0.6625\n",
      "Epoch 28/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.1312 - accuracy: 0.8319 - custom__precision: 0.8170 - custom__recall: 0.8542\n",
      "Epoch 28: val_loss did not improve from 0.13161\n",
      "791/791 [==============================] - 22s 28ms/step - loss: 0.1312 - accuracy: 0.8319 - custom__precision: 0.8170 - custom__recall: 0.8542 - val_loss: 0.1380 - val_accuracy: 0.8130 - val_custom__precision: 0.8834 - val_custom__recall: 0.7213\n",
      "Epoch 29/200\n",
      "789/791 [============================>.] - ETA: 0s - loss: 0.1279 - accuracy: 0.8351 - custom__precision: 0.8255 - custom__recall: 0.8508\n",
      "Epoch 29: val_loss improved from 0.13161 to 0.12520, saving model to weights/best_fit.hdf5\n",
      "791/791 [==============================] - 23s 29ms/step - loss: 0.1278 - accuracy: 0.8352 - custom__precision: 0.8259 - custom__recall: 0.8506 - val_loss: 0.1252 - val_accuracy: 0.8277 - val_custom__precision: 0.8031 - val_custom__recall: 0.8683\n",
      "Epoch 30/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.1304 - accuracy: 0.8277 - custom__precision: 0.8133 - custom__recall: 0.8502\n",
      "Epoch 30: val_loss did not improve from 0.12520\n",
      "791/791 [==============================] - 22s 27ms/step - loss: 0.1303 - accuracy: 0.8279 - custom__precision: 0.8135 - custom__recall: 0.8503 - val_loss: 0.1351 - val_accuracy: 0.8172 - val_custom__precision: 0.8794 - val_custom__recall: 0.7353\n",
      "Epoch 31/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.1281 - accuracy: 0.8396 - custom__precision: 0.8250 - custom__recall: 0.8621\n",
      "Epoch 31: val_loss did not improve from 0.12520\n",
      "791/791 [==============================] - 24s 30ms/step - loss: 0.1282 - accuracy: 0.8394 - custom__precision: 0.8250 - custom__recall: 0.8620 - val_loss: 0.1337 - val_accuracy: 0.8228 - val_custom__precision: 0.8607 - val_custom__recall: 0.7703\n",
      "Epoch 32/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.1285 - accuracy: 0.8352 - custom__precision: 0.8211 - custom__recall: 0.8572\n",
      "Epoch 32: val_loss did not improve from 0.12520\n",
      "791/791 [==============================] - 22s 28ms/step - loss: 0.1285 - accuracy: 0.8352 - custom__precision: 0.8211 - custom__recall: 0.8572 - val_loss: 0.1436 - val_accuracy: 0.8018 - val_custom__precision: 0.8897 - val_custom__recall: 0.6891\n",
      "Epoch 33/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.1274 - accuracy: 0.8404 - custom__precision: 0.8330 - custom__recall: 0.8504\n",
      "Epoch 33: val_loss did not improve from 0.12520\n",
      "791/791 [==============================] - 24s 30ms/step - loss: 0.1274 - accuracy: 0.8404 - custom__precision: 0.8330 - custom__recall: 0.8504 - val_loss: 0.1265 - val_accuracy: 0.8256 - val_custom__precision: 0.8326 - val_custom__recall: 0.8151\n",
      "Epoch 34/200\n",
      "789/791 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.8395 - custom__precision: 0.8309 - custom__recall: 0.8543\n",
      "Epoch 34: val_loss did not improve from 0.12520\n",
      "791/791 [==============================] - 22s 28ms/step - loss: 0.1273 - accuracy: 0.8388 - custom__precision: 0.8304 - custom__recall: 0.8534 - val_loss: 0.1370 - val_accuracy: 0.8074 - val_custom__precision: 0.8871 - val_custom__recall: 0.7045\n",
      "Epoch 35/200\n",
      "  4/791 [..............................] - ETA: 16s - loss: 0.1528 - accuracy: 0.7500 - custom__precision: 0.6250 - custom__recall: 0.8333"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(model_file, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m tensorboard_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39mlog_dir, histogram_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43msiamese_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                            \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining_pairs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE_TRAIN\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tfvenv/lib64/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/tfvenv/lib64/python3.9/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/tfvenv/lib64/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/tfvenv/lib64/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/tfvenv/lib64/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/tfvenv/lib64/python3.9/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tfvenv/lib64/python3.9/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/tfvenv/lib64/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/tfvenv/lib64/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "model_file = \"weights/best_fit.hdf5\"\n",
    "log_dir = \"logs_new\"\n",
    "\n",
    "siamese_model.compile(loss=contrastive_loss, \n",
    "                      optimizer=tf.keras.optimizers.Adam(),\n",
    "                      metrics=[Custom_Accuracy(), Custom_Precision(), Custom_Recall()]\n",
    "                      )\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 30, restore_best_weights = False)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(model_file, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\n",
    "\n",
    "history = siamese_model.fit(train_dataset, \n",
    "                            steps_per_epoch=math.ceil(len(training_pairs) / BATCH_SIZE_TRAIN), \n",
    "                            validation_data=validation_dataset, \n",
    "                            epochs = EPOCHS,\n",
    "                            callbacks=[early_stop, checkpoint, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, pair in enumerate(validation_pairs):\n",
    "    imgA, imgB = preprocess_pair(pair)\n",
    "\n",
    "    # Add batch dimension\n",
    "    imgA = tf.expand_dims(imgA, axis=0)  # (1, 224, 224, 3)\n",
    "    imgB = tf.expand_dims(imgB, axis=0)  # (1, 224, 224, 3)\n",
    "\n",
    "    prediction = siamese_model.predict([imgA, imgB])  \n",
    "    print(f\"Distance: {prediction[0]}\")\n",
    "    print(f\"Predicted: {prediction[0] <= 0.5}\")\n",
    "    print(f\"Label: {bool(validation_pairs_labels[index])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
