{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 17:23:12.417549: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-10 17:23:12.514832: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-10 17:23:12.537234: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-10 17:23:14.066324: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:\n",
      "2025-02-10 17:23:14.066407: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:\n",
      "2025-02-10 17:23:14.066414: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 17:23:18.309987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-10 17:23:18.318292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-10 17:23:18.318395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "!source /etc/profile.d/modules.sh\n",
    "!module load CUDA/11.2\n",
    "!export PATH=/local/java/cuda-11.2/bin:$PATH\n",
    "!export LD_LIBRARY_PATH=/local/java/cuda-11.2/lib64:/local/java/cudnn-8.1_for_cuda_11.2/lib64:$LD_LIBRARY_PATH  # this line is needed for it to recognise gpu devices -- run this in the terminal\n",
    "!export CUDA_HOME=/local/java/cuda-11.2\n",
    "import os \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten, Dropout, GlobalAveragePooling2D, Lambda\n",
    "from datetime import datetime\n",
    "from keras import backend as K\n",
    "\n",
    "print(tf.__version__)  # 2.10.0\n",
    "print(tf.config.list_physical_devices('GPU'))  # should show gpu available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 17:23:18.330822: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-10 17:23:18.331492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-10 17:23:18.331617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-10 17:23:18.331677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-10 17:23:18.634399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-10 17:23:18.634528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-10 17:23:18.634597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-10 17:23:18.634660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9474 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "def preprocess(image_path):\n",
    "    # read the file \n",
    "    raw = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_image(raw, expand_animations=False, channels = 3)\n",
    "    img = tf.image.resize(img, size = (224, 224), preserve_aspect_ratio=True)\n",
    "    img = tf.image.resize_with_crop_or_pad(img, 224, 224)\n",
    "    img = tf.cast(img, tf.float32)/255.0\n",
    "    return img \n",
    "\n",
    "def preprocess_pair(pair):\n",
    "    imgA = preprocess(pair[0])\n",
    "    imgB = preprocess(pair[1])\n",
    "    return (imgA, imgB)\n",
    "\n",
    "class RandomInvert(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_value = 255, factor=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.factor = factor\n",
    "        self.max_value = max_value\n",
    "\n",
    "    def call(self, x):\n",
    "        if  tf.random.uniform([]) < self.factor:\n",
    "            x = (self.max_value - x)\n",
    "        return x\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    RandomInvert(max_value = 1.0),\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomRotation((-0.4, 0.4)),\n",
    "    tf.keras.layers.RandomBrightness(factor=(-0.2, 0.2), value_range=(0., 1.)),\n",
    "    tf.keras.layers.GaussianNoise(0.005),\n",
    "    tf.keras.layers.RandomZoom(height_factor=(-0.4, 0.4)),\n",
    "    tf.keras.layers.RandomContrast(factor=(0.1, 0.9)),\n",
    "    tf.keras.layers.RandomTranslation(height_factor=0.2, width_factor=0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (224, 224, 3)\n",
    "BATCH_SIZE_TRAIN = 8\n",
    "BATCH_SIZE_VALIDATION = 2\n",
    "MARGIN = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 96 images.\n",
      "8 classes:  ['alarm_clock' 'water_bottle' 'camera' 'mouse' 'digital_watch' 'wallet'\n",
      " 'backpack' 'pencil_case']\n",
      "74\n",
      "74\n",
      "22\n",
      "22\n",
      "['train_separated/alarm_clock/20250209_201734.jpg', 'train_separated/alarm_clock/20250204_171515.jpg', 'train_separated/alarm_clock/20250209_202239.jpg', 'train_separated/alarm_clock/20250204_173420.jpg']\n",
      "[0, 0, 0, 0]\n",
      "['train_separated/alarm_clock/20250209_202700.jpg', 'train_separated/alarm_clock/20250209_201749.jpg', 'train_separated/alarm_clock/20250209_202756.jpg', 'train_separated/alarm_clock/20250209_202815.jpg']\n",
      "[0, 0, 0, 0]\n",
      "74\n",
      "22\n",
      "80\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"flowers102\"\n",
    "\n",
    "files = [os.path.join(r,file) for r,d,f in os.walk(dataset_path) for file in f]\n",
    "file_size = len(files)\n",
    "print(\"We have \" + str(file_size) + \" images.\")\n",
    "random.shuffle(files)\n",
    "\n",
    "def make_instances(files, classes = []):\n",
    "    labels = []\n",
    "    for i in range(len(files)):\n",
    "        file = files[i]\n",
    "        clazz = file.split(\"/\")[-2]\n",
    "        \n",
    "        if classes.count(clazz):\n",
    "            label = classes.index(clazz)\n",
    "        else:\n",
    "            label = len(classes)\n",
    "            classes.append(clazz)\n",
    "\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(labels), np.array(classes)\n",
    "\n",
    "labels, classes = make_instances(files)\n",
    "CLASSES_SIZE = len(classes)\n",
    "\n",
    "print(str(CLASSES_SIZE) + \" classes: \", classes)\n",
    "\n",
    "training_files = []\n",
    "training_labels = []\n",
    "validation_files = []\n",
    "validation_labels = []\n",
    "\n",
    "for label in range(CLASSES_SIZE):\n",
    "    indexes = np.where(labels == label)[0]\n",
    "\n",
    "    threshold = len(indexes) * 80 // 100\n",
    "\n",
    "    training_indexes = indexes[0:threshold]\n",
    "    training_files_for_class = [files[i] for i in training_indexes]\n",
    "\n",
    "    training_files.extend(training_files_for_class)\n",
    "    training_labels.extend([label] * len(training_files_for_class))\n",
    "\n",
    "    validation_indexes = indexes[threshold:]\n",
    "    validation_files_for_class = [files[i] for i in validation_indexes]\n",
    "\n",
    "    validation_files.extend(validation_files_for_class)\n",
    "    validation_labels.extend([label] * len(validation_files_for_class))\n",
    "\n",
    "print(len(training_files))\n",
    "print(len(training_labels))\n",
    "print(len(validation_files))\n",
    "print(len(validation_labels))\n",
    "\n",
    "print(training_files[0:4])\n",
    "print(training_labels[0:4])\n",
    "\n",
    "print(validation_files[0:4])\n",
    "print(validation_labels[0:4])\n",
    "\n",
    "def create_pairs(x, digit_indices):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "\n",
    "    n = min([len(digit_indices[d]) for d in range(CLASSES_SIZE)]) - 1\n",
    "    \n",
    "    for d in range(CLASSES_SIZE):\n",
    "        for i in range(n):\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            inc = random.randrange(1, CLASSES_SIZE)\n",
    "            dn = (d + inc) % CLASSES_SIZE\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            labels += [1, 0]\n",
    "            \n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "\n",
    "def create_pairs_on_set(images, labels):\n",
    "    \n",
    "    digit_indices = [np.where(labels == i)[0] for i in range(CLASSES_SIZE)]\n",
    "    pairs, y = create_pairs(images, digit_indices)\n",
    "    y = y.astype('float32')\n",
    "    \n",
    "    return pairs, y\n",
    "\n",
    "training_files = np.array(training_files)\n",
    "training_labels = np.array(training_labels)\n",
    "\n",
    "validation_files = np.array(validation_files)\n",
    "validation_labels = np.array(validation_labels)\n",
    "\n",
    "training_pairs, training_pairs_labels = create_pairs_on_set(training_files, training_labels)\n",
    "validation_pairs, validation_pairs_labels = create_pairs_on_set(validation_files, validation_labels)\n",
    "\n",
    "print(len(training_files))\n",
    "print(len(validation_files))\n",
    "print(len(training_pairs))\n",
    "print(len(validation_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "def build_training_dataset():\n",
    "\n",
    "    pairs_tensor = tf.convert_to_tensor(training_pairs)\n",
    "    labels_tensor = tf.convert_to_tensor(training_pairs_labels)\n",
    "\n",
    "    result = tf.data.Dataset.from_tensor_slices((pairs_tensor, labels_tensor))\n",
    "\n",
    "    result = result.map(lambda pair, label: (preprocess_pair(pair), label))\n",
    "    result = result.shuffle(128, reshuffle_each_iteration=True)\n",
    "    result = result.repeat()\n",
    "    result = result.batch(BATCH_SIZE_TRAIN)\n",
    "    result = result.map(lambda pair, y: ((data_augmentation(pair[0], training=True),data_augmentation(pair[1], training=True)), y), \n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    result = result.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return result\n",
    "\n",
    "train_dataset = build_training_dataset()\n",
    "\n",
    "def build_validation_dataset():\n",
    "\n",
    "    pairs_tensor = tf.convert_to_tensor(validation_pairs)\n",
    "    labels_tensor = tf.convert_to_tensor(validation_pairs_labels)\n",
    "\n",
    "    result = tf.data.Dataset.from_tensor_slices((pairs_tensor, labels_tensor))\n",
    "\n",
    "    result = result.map(lambda pair, label: (preprocess_pair(pair), label))\n",
    "    result = result.batch(BATCH_SIZE_VALIDATION)\n",
    "    result = result.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return result\n",
    "\n",
    "validation_dataset = build_validation_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vectors):\n",
    "    x, y = vectors\n",
    "    sum_squared = K.sum(K.square(x-y), axis = 1, keepdims= True)\n",
    "    return K.sqrt(K.maximum(sum_squared, K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make embedding\n",
    "embedding is each sub, identical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding():\n",
    "    inputs = tf.keras.layers.Input(INPUT_SHAPE)\n",
    "    base_model = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=INPUT_SHAPE, include_top=False, weights='imagenet')\n",
    "    \n",
    "    base_model.trainable = True\n",
    "    limit = len(base_model.layers)-int(len(base_model.layers)*.10)\n",
    "    for layer in base_model.layers[:limit]:\n",
    "        layer.trainable =  False\n",
    "          \n",
    "    x = base_model(inputs)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs=tf.keras.layers.Dense(64)(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model():\n",
    "    # create embedding\n",
    "    embedding = make_embedding()\n",
    "\n",
    "    # create the same embedding for the two inputs \n",
    "    input_a = Input(shape = INPUT_SHAPE, name = \"first_image\")\n",
    "    input_b = Input(shape = INPUT_SHAPE, name = \"second_image\")\n",
    "\n",
    "    embedding_a = embedding(input_a)\n",
    "    embedding_b = embedding(input_b)\n",
    "\n",
    "    # Create the final euclidean distance layer\n",
    "    output = Lambda(euclidean_distance, name = \"distance\")([embedding_a, embedding_b])\n",
    "\n",
    "    return Model([input_a, input_b], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " first_image (InputLayer)       [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " second_image (InputLayer)      [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 64)           2339968     ['first_image[0][0]',            \n",
      "                                                                  'second_image[0][0]']           \n",
      "                                                                                                  \n",
      " distance (Lambda)              (None, 1)            0           ['model[0][0]',                  \n",
      "                                                                  'model[1][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,339,968\n",
      "Trainable params: 1,121,984\n",
      "Non-trainable params: 1,217,984\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_model = make_siamese_model()\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(MARGIN - y_pred, 0))\n",
    "    return (y_true * square_pred + (1 - y_true) * margin_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Precision(tf.keras.metrics.Precision):\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_fix = tf.math.less(y_pred, 0.5) \n",
    "        y_pred_fix = tf.cast(y_pred_fix, y_pred.dtype)\n",
    "       \n",
    "        return super().update_state(y_true, y_pred_fix, sample_weight)\n",
    "  \n",
    "class Custom_Recall(tf.keras.metrics.Recall):\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_fix = tf.math.less(y_pred, 0.5) \n",
    "        y_pred_fix = tf.cast(y_pred_fix, y_pred.dtype)\n",
    "       \n",
    "        return super().update_state(y_true, y_pred_fix, sample_weight)\n",
    "  \n",
    "class Custom_Accuracy(tf.keras.metrics.Accuracy):\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_fix = tf.math.less(y_pred, 0.5) \n",
    "        y_pred_fix = tf.cast(y_pred_fix, y_pred.dtype)\n",
    "       \n",
    "        return super().update_state(y_true, y_pred_fix, sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 17:23:28.966653: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2025-02-10 17:23:29.363374: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-02-10 17:23:29.363775: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-02-10 17:23:29.363789: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2025-02-10 17:23:29.364152: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-02-10 17:23:29.364179: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2025-02-10 17:23:29.632377: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 9.7989 - accuracy: 0.5000 - custom__precision: 0.0000e+00 - custom__recall: 0.0000e+00 \n",
      "Epoch 1: val_loss improved from inf to 38.45818, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 7s 185ms/step - loss: 9.7989 - accuracy: 0.5000 - custom__precision: 0.0000e+00 - custom__recall: 0.0000e+00 - val_loss: 38.4582 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 2/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0364 - accuracy: 0.5125 - custom__precision: 1.0000 - custom__recall: 0.0250        \n",
      "Epoch 2: val_loss improved from 38.45818 to 30.99074, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.0364 - accuracy: 0.5125 - custom__precision: 1.0000 - custom__recall: 0.0250 - val_loss: 30.9907 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 3/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4498 - accuracy: 0.5417 - custom__precision: 0.6111 - custom__recall: 0.2973\n",
      "Epoch 3: val_loss improved from 30.99074 to 27.66538, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.4267 - accuracy: 0.5375 - custom__precision: 0.5652 - custom__recall: 0.3250 - val_loss: 27.6654 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 4/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.2868 - accuracy: 0.5139 - custom__precision: 0.5484 - custom__recall: 0.4474\n",
      "Epoch 4: val_loss improved from 27.66538 to 25.37540, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2803 - accuracy: 0.5375 - custom__precision: 0.5429 - custom__recall: 0.4750 - val_loss: 25.3754 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 5/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.2600 - accuracy: 0.6250 - custom__precision: 0.6047 - custom__recall: 0.7222\n",
      "Epoch 5: val_loss improved from 25.37540 to 22.88841, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2593 - accuracy: 0.6125 - custom__precision: 0.5918 - custom__recall: 0.7250 - val_loss: 22.8884 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 6/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.2577 - accuracy: 0.6111 - custom__precision: 0.5946 - custom__recall: 0.6286\n",
      "Epoch 6: val_loss improved from 22.88841 to 20.50494, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 2s 179ms/step - loss: 0.2524 - accuracy: 0.6125 - custom__precision: 0.6000 - custom__recall: 0.6750 - val_loss: 20.5049 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 7/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.3166 - accuracy: 0.5417 - custom__precision: 0.5000 - custom__recall: 0.6364\n",
      "Epoch 7: val_loss improved from 20.50494 to 18.26278, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 2s 187ms/step - loss: 0.3038 - accuracy: 0.5750 - custom__precision: 0.5600 - custom__recall: 0.7000 - val_loss: 18.2628 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 8/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.2713 - accuracy: 0.6111 - custom__precision: 0.6098 - custom__recall: 0.6757\n",
      "Epoch 8: val_loss improved from 18.26278 to 16.24807, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 149ms/step - loss: 0.2629 - accuracy: 0.6250 - custom__precision: 0.6087 - custom__recall: 0.7000 - val_loss: 16.2481 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 9/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.2807 - accuracy: 0.5694 - custom__precision: 0.5641 - custom__recall: 0.6111\n",
      "Epoch 9: val_loss improved from 16.24807 to 14.19291, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 142ms/step - loss: 0.2704 - accuracy: 0.5750 - custom__precision: 0.5750 - custom__recall: 0.5750 - val_loss: 14.1929 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 10/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2615 - accuracy: 0.6000 - custom__precision: 0.5909 - custom__recall: 0.6500\n",
      "Epoch 10: val_loss improved from 14.19291 to 12.33560, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 153ms/step - loss: 0.2615 - accuracy: 0.6000 - custom__precision: 0.5909 - custom__recall: 0.6500 - val_loss: 12.3356 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 11/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.2322 - accuracy: 0.6528 - custom__precision: 0.6136 - custom__recall: 0.7714\n",
      "Epoch 11: val_loss improved from 12.33560 to 11.01462, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.2870 - accuracy: 0.6250 - custom__precision: 0.6136 - custom__recall: 0.6750 - val_loss: 11.0146 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 12/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4466 - accuracy: 0.4722 - custom__precision: 0.4444 - custom__recall: 0.4706\n",
      "Epoch 12: val_loss improved from 11.01462 to 9.51789, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 135ms/step - loss: 0.4149 - accuracy: 0.5125 - custom__precision: 0.5122 - custom__recall: 0.5250 - val_loss: 9.5179 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 13/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.2619 - accuracy: 0.6111 - custom__precision: 0.6000 - custom__recall: 0.7297\n",
      "Epoch 13: val_loss improved from 9.51789 to 8.30685, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 146ms/step - loss: 0.2547 - accuracy: 0.6250 - custom__precision: 0.6000 - custom__recall: 0.7500 - val_loss: 8.3069 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 14/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.2408 - accuracy: 0.5972 - custom__precision: 0.5952 - custom__recall: 0.6757\n",
      "Epoch 14: val_loss improved from 8.30685 to 7.38865, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 138ms/step - loss: 0.2533 - accuracy: 0.5625 - custom__precision: 0.5556 - custom__recall: 0.6250 - val_loss: 7.3886 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 15/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.2840 - accuracy: 0.5833 - custom__precision: 0.6129 - custom__recall: 0.5135\n",
      "Epoch 15: val_loss improved from 7.38865 to 6.51561, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2710 - accuracy: 0.6000 - custom__precision: 0.6176 - custom__recall: 0.5250 - val_loss: 6.5156 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 16/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3649 - accuracy: 0.5625 - custom__precision: 0.5676 - custom__recall: 0.5250\n",
      "Epoch 16: val_loss improved from 6.51561 to 6.38223, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 132ms/step - loss: 0.3649 - accuracy: 0.5625 - custom__precision: 0.5676 - custom__recall: 0.5250 - val_loss: 6.3822 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 17/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.3483 - accuracy: 0.5556 - custom__precision: 0.5278 - custom__recall: 0.5588\n",
      "Epoch 17: val_loss improved from 6.38223 to 4.68493, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 0.3318 - accuracy: 0.5750 - custom__precision: 0.5750 - custom__recall: 0.5750 - val_loss: 4.6849 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 18/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.2559 - accuracy: 0.5972 - custom__precision: 0.5745 - custom__recall: 0.7500\n",
      "Epoch 18: val_loss improved from 4.68493 to 3.99130, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 135ms/step - loss: 0.2577 - accuracy: 0.5750 - custom__precision: 0.5577 - custom__recall: 0.7250 - val_loss: 3.9913 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 19/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.2398 - accuracy: 0.6528 - custom__precision: 0.6383 - custom__recall: 0.7895\n",
      "Epoch 19: val_loss improved from 3.99130 to 3.71248, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 135ms/step - loss: 0.2455 - accuracy: 0.6250 - custom__precision: 0.5962 - custom__recall: 0.7750 - val_loss: 3.7125 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 20/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.2883 - accuracy: 0.6667 - custom__precision: 0.6875 - custom__recall: 0.6111\n",
      "Epoch 20: val_loss did not improve from 3.71248\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.2924 - accuracy: 0.6625 - custom__precision: 0.6857 - custom__recall: 0.6000 - val_loss: 3.8908 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 21/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4054 - accuracy: 0.6125 - custom__precision: 0.6098 - custom__recall: 0.6250\n",
      "Epoch 21: val_loss improved from 3.71248 to 3.01276, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 2s 165ms/step - loss: 0.4054 - accuracy: 0.6125 - custom__precision: 0.6098 - custom__recall: 0.6250 - val_loss: 3.0128 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 22/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3031 - accuracy: 0.6125 - custom__precision: 0.6154 - custom__recall: 0.6000\n",
      "Epoch 22: val_loss improved from 3.01276 to 2.50260, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.3031 - accuracy: 0.6125 - custom__precision: 0.6154 - custom__recall: 0.6000 - val_loss: 2.5026 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 23/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2309 - accuracy: 0.6750 - custom__precision: 0.6400 - custom__recall: 0.8000\n",
      "Epoch 23: val_loss improved from 2.50260 to 2.02884, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2309 - accuracy: 0.6750 - custom__precision: 0.6400 - custom__recall: 0.8000 - val_loss: 2.0288 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 24/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2281 - accuracy: 0.6750 - custom__precision: 0.6400 - custom__recall: 0.8000\n",
      "Epoch 24: val_loss improved from 2.02884 to 1.60487, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2281 - accuracy: 0.6750 - custom__precision: 0.6400 - custom__recall: 0.8000 - val_loss: 1.6049 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 25/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2323 - accuracy: 0.6875 - custom__precision: 0.6596 - custom__recall: 0.7750\n",
      "Epoch 25: val_loss did not improve from 1.60487\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.2323 - accuracy: 0.6875 - custom__precision: 0.6596 - custom__recall: 0.7750 - val_loss: 1.8678 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 26/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2811 - accuracy: 0.6375 - custom__precision: 0.6571 - custom__recall: 0.5750\n",
      "Epoch 26: val_loss improved from 1.60487 to 1.48854, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 154ms/step - loss: 0.2811 - accuracy: 0.6375 - custom__precision: 0.6571 - custom__recall: 0.5750 - val_loss: 1.4885 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 27/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2540 - accuracy: 0.6875 - custom__precision: 0.6829 - custom__recall: 0.7000\n",
      "Epoch 27: val_loss improved from 1.48854 to 1.02254, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.2540 - accuracy: 0.6875 - custom__precision: 0.6829 - custom__recall: 0.7000 - val_loss: 1.0225 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 28/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.6250 - custom__precision: 0.6087 - custom__recall: 0.7000\n",
      "Epoch 28: val_loss improved from 1.02254 to 0.85263, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 133ms/step - loss: 0.2647 - accuracy: 0.6250 - custom__precision: 0.6087 - custom__recall: 0.7000 - val_loss: 0.8526 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 29/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2320 - accuracy: 0.6500 - custom__precision: 0.6250 - custom__recall: 0.7500\n",
      "Epoch 29: val_loss improved from 0.85263 to 0.78315, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 0.2320 - accuracy: 0.6500 - custom__precision: 0.6250 - custom__recall: 0.7500 - val_loss: 0.7831 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 30/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2585 - accuracy: 0.6250 - custom__precision: 0.6042 - custom__recall: 0.7250\n",
      "Epoch 30: val_loss did not improve from 0.78315\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.2585 - accuracy: 0.6250 - custom__precision: 0.6042 - custom__recall: 0.7250 - val_loss: 0.9259 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 31/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4098 - accuracy: 0.5500 - custom__precision: 0.5556 - custom__recall: 0.5000\n",
      "Epoch 31: val_loss improved from 0.78315 to 0.46216, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 0.4098 - accuracy: 0.5500 - custom__precision: 0.5556 - custom__recall: 0.5000 - val_loss: 0.4622 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 32/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2339 - accuracy: 0.6875 - custom__precision: 0.6923 - custom__recall: 0.6750\n",
      "Epoch 32: val_loss improved from 0.46216 to 0.42456, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 2s 190ms/step - loss: 0.2339 - accuracy: 0.6875 - custom__precision: 0.6923 - custom__recall: 0.6750 - val_loss: 0.4246 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 33/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2562 - accuracy: 0.6250 - custom__precision: 0.6250 - custom__recall: 0.6250\n",
      "Epoch 33: val_loss improved from 0.42456 to 0.40048, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 130ms/step - loss: 0.2562 - accuracy: 0.6250 - custom__precision: 0.6250 - custom__recall: 0.6250 - val_loss: 0.4005 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 34/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2416 - accuracy: 0.5750 - custom__precision: 0.5714 - custom__recall: 0.6000\n",
      "Epoch 34: val_loss did not improve from 0.40048\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2416 - accuracy: 0.5750 - custom__precision: 0.5714 - custom__recall: 0.6000 - val_loss: 0.5307 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 35/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3167 - accuracy: 0.6250 - custom__precision: 0.6389 - custom__recall: 0.5750\n",
      "Epoch 35: val_loss improved from 0.40048 to 0.37197, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3167 - accuracy: 0.6250 - custom__precision: 0.6389 - custom__recall: 0.5750 - val_loss: 0.3720 - val_accuracy: 0.5000 - val_custom__precision: 0.0000e+00 - val_custom__recall: 0.0000e+00\n",
      "Epoch 36/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.5875 - custom__precision: 0.5854 - custom__recall: 0.6000\n",
      "Epoch 36: val_loss improved from 0.37197 to 0.33612, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 156ms/step - loss: 0.2594 - accuracy: 0.5875 - custom__precision: 0.5854 - custom__recall: 0.6000 - val_loss: 0.3361 - val_accuracy: 0.5625 - val_custom__precision: 1.0000 - val_custom__recall: 0.1250\n",
      "Epoch 37/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.6250 - custom__precision: 0.6389 - custom__recall: 0.5750\n",
      "Epoch 37: val_loss improved from 0.33612 to 0.29047, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 137ms/step - loss: 0.2492 - accuracy: 0.6250 - custom__precision: 0.6389 - custom__recall: 0.5750 - val_loss: 0.2905 - val_accuracy: 0.5625 - val_custom__precision: 1.0000 - val_custom__recall: 0.1250\n",
      "Epoch 38/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.6750 - custom__precision: 0.6842 - custom__recall: 0.6500\n",
      "Epoch 38: val_loss improved from 0.29047 to 0.28917, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 2s 165ms/step - loss: 0.2516 - accuracy: 0.6750 - custom__precision: 0.6842 - custom__recall: 0.6500 - val_loss: 0.2892 - val_accuracy: 0.6250 - val_custom__precision: 1.0000 - val_custom__recall: 0.2500\n",
      "Epoch 39/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2824 - accuracy: 0.6250 - custom__precision: 0.6316 - custom__recall: 0.6000\n",
      "Epoch 39: val_loss did not improve from 0.28917\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.2824 - accuracy: 0.6250 - custom__precision: 0.6316 - custom__recall: 0.6000 - val_loss: 0.5518 - val_accuracy: 0.6250 - val_custom__precision: 1.0000 - val_custom__recall: 0.2500\n",
      "Epoch 40/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3243 - accuracy: 0.6375 - custom__precision: 0.6667 - custom__recall: 0.5500\n",
      "Epoch 40: val_loss improved from 0.28917 to 0.26070, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.3243 - accuracy: 0.6375 - custom__precision: 0.6667 - custom__recall: 0.5500 - val_loss: 0.2607 - val_accuracy: 0.6250 - val_custom__precision: 1.0000 - val_custom__recall: 0.2500\n",
      "Epoch 41/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2302 - accuracy: 0.6375 - custom__precision: 0.6122 - custom__recall: 0.7500\n",
      "Epoch 41: val_loss did not improve from 0.26070\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.2302 - accuracy: 0.6375 - custom__precision: 0.6122 - custom__recall: 0.7500 - val_loss: 0.3640 - val_accuracy: 0.6250 - val_custom__precision: 1.0000 - val_custom__recall: 0.2500\n",
      "Epoch 42/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2086 - accuracy: 0.6875 - custom__precision: 0.6364 - custom__recall: 0.8750\n",
      "Epoch 42: val_loss did not improve from 0.26070\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.2086 - accuracy: 0.6875 - custom__precision: 0.6364 - custom__recall: 0.8750 - val_loss: 0.3074 - val_accuracy: 0.5625 - val_custom__precision: 0.6667 - val_custom__recall: 0.2500\n",
      "Epoch 43/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2302 - accuracy: 0.6375 - custom__precision: 0.6486 - custom__recall: 0.6000\n",
      "Epoch 43: val_loss improved from 0.26070 to 0.22905, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 150ms/step - loss: 0.2302 - accuracy: 0.6375 - custom__precision: 0.6486 - custom__recall: 0.6000 - val_loss: 0.2290 - val_accuracy: 0.5000 - val_custom__precision: 0.5000 - val_custom__recall: 0.2500\n",
      "Epoch 44/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1937 - accuracy: 0.6500 - custom__precision: 0.6200 - custom__recall: 0.7750\n",
      "Epoch 44: val_loss did not improve from 0.22905\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.1937 - accuracy: 0.6500 - custom__precision: 0.6200 - custom__recall: 0.7750 - val_loss: 0.2668 - val_accuracy: 0.5000 - val_custom__precision: 0.5000 - val_custom__recall: 0.2500\n",
      "Epoch 45/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2155 - accuracy: 0.6625 - custom__precision: 0.6444 - custom__recall: 0.7250      \n",
      "Epoch 45: val_loss improved from 0.22905 to 0.21922, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.2155 - accuracy: 0.6625 - custom__precision: 0.6444 - custom__recall: 0.7250 - val_loss: 0.2192 - val_accuracy: 0.5000 - val_custom__precision: 0.5000 - val_custom__recall: 0.2500\n",
      "Epoch 46/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.6500 - custom__precision: 0.6250 - custom__recall: 0.7500\n",
      "Epoch 46: val_loss did not improve from 0.21922\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.2226 - accuracy: 0.6500 - custom__precision: 0.6250 - custom__recall: 0.7500 - val_loss: 0.2223 - val_accuracy: 0.5000 - val_custom__precision: 0.5000 - val_custom__recall: 0.2500\n",
      "Epoch 47/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1897 - accuracy: 0.7250 - custom__precision: 0.6731 - custom__recall: 0.8750\n",
      "Epoch 47: val_loss did not improve from 0.21922\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.1897 - accuracy: 0.7250 - custom__precision: 0.6731 - custom__recall: 0.8750 - val_loss: 0.2639 - val_accuracy: 0.6250 - val_custom__precision: 1.0000 - val_custom__recall: 0.2500\n",
      "Epoch 48/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.6250 - custom__precision: 0.6316 - custom__recall: 0.6000\n",
      "Epoch 48: val_loss did not improve from 0.21922\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.2454 - accuracy: 0.6250 - custom__precision: 0.6316 - custom__recall: 0.6000 - val_loss: 0.3926 - val_accuracy: 0.6250 - val_custom__precision: 1.0000 - val_custom__recall: 0.2500\n",
      "Epoch 49/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2288 - accuracy: 0.6625 - custom__precision: 0.6667 - custom__recall: 0.6500\n",
      "Epoch 49: val_loss did not improve from 0.21922\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.2288 - accuracy: 0.6625 - custom__precision: 0.6667 - custom__recall: 0.6500 - val_loss: 0.2510 - val_accuracy: 0.6875 - val_custom__precision: 1.0000 - val_custom__recall: 0.3750\n",
      "Epoch 50/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2676 - accuracy: 0.5625 - custom__precision: 0.5641 - custom__recall: 0.5500\n",
      "Epoch 50: val_loss did not improve from 0.21922\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.2676 - accuracy: 0.5625 - custom__precision: 0.5641 - custom__recall: 0.5500 - val_loss: 0.2546 - val_accuracy: 0.5625 - val_custom__precision: 0.6667 - val_custom__recall: 0.2500\n",
      "Epoch 51/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2105 - accuracy: 0.7000 - custom__precision: 0.6905 - custom__recall: 0.7250\n",
      "Epoch 51: val_loss improved from 0.21922 to 0.21412, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 2s 190ms/step - loss: 0.2105 - accuracy: 0.7000 - custom__precision: 0.6905 - custom__recall: 0.7250 - val_loss: 0.2141 - val_accuracy: 0.6875 - val_custom__precision: 0.8000 - val_custom__recall: 0.5000\n",
      "Epoch 52/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2730 - accuracy: 0.5625 - custom__precision: 0.5455 - custom__recall: 0.7500\n",
      "Epoch 52: val_loss improved from 0.21412 to 0.20235, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.2730 - accuracy: 0.5625 - custom__precision: 0.5455 - custom__recall: 0.7500 - val_loss: 0.2023 - val_accuracy: 0.6875 - val_custom__precision: 0.8000 - val_custom__recall: 0.5000\n",
      "Epoch 53/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2181 - accuracy: 0.7125 - custom__precision: 0.6809 - custom__recall: 0.8000\n",
      "Epoch 53: val_loss improved from 0.20235 to 0.20224, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.2181 - accuracy: 0.7125 - custom__precision: 0.6809 - custom__recall: 0.8000 - val_loss: 0.2022 - val_accuracy: 0.5625 - val_custom__precision: 0.5556 - val_custom__recall: 0.6250\n",
      "Epoch 54/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2450 - accuracy: 0.6000 - custom__precision: 0.5909 - custom__recall: 0.6500\n",
      "Epoch 54: val_loss improved from 0.20224 to 0.19173, saving model to weights/best_fit.hdf5\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.2450 - accuracy: 0.6000 - custom__precision: 0.5909 - custom__recall: 0.6500 - val_loss: 0.1917 - val_accuracy: 0.6875 - val_custom__precision: 0.8000 - val_custom__recall: 0.5000\n",
      "Epoch 55/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2082 - accuracy: 0.6750 - custom__precision: 0.6750 - custom__recall: 0.6750\n",
      "Epoch 55: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.2082 - accuracy: 0.6750 - custom__precision: 0.6750 - custom__recall: 0.6750 - val_loss: 0.2498 - val_accuracy: 0.6250 - val_custom__precision: 1.0000 - val_custom__recall: 0.2500\n",
      "Epoch 56/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2645 - accuracy: 0.6500 - custom__precision: 0.6304 - custom__recall: 0.7250\n",
      "Epoch 56: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.2645 - accuracy: 0.6500 - custom__precision: 0.6304 - custom__recall: 0.7250 - val_loss: 0.3923 - val_accuracy: 0.6250 - val_custom__precision: 1.0000 - val_custom__recall: 0.2500\n",
      "Epoch 57/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2753 - accuracy: 0.7000 - custom__precision: 0.7105 - custom__recall: 0.6750\n",
      "Epoch 57: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.2753 - accuracy: 0.7000 - custom__precision: 0.7105 - custom__recall: 0.6750 - val_loss: 0.2507 - val_accuracy: 0.6875 - val_custom__precision: 1.0000 - val_custom__recall: 0.3750\n",
      "Epoch 58/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.7000 - custom__precision: 0.7353 - custom__recall: 0.6250\n",
      "Epoch 58: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.2536 - accuracy: 0.7000 - custom__precision: 0.7353 - custom__recall: 0.6250 - val_loss: 0.4653 - val_accuracy: 0.6250 - val_custom__precision: 1.0000 - val_custom__recall: 0.2500\n",
      "Epoch 59/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3952 - accuracy: 0.6500 - custom__precision: 0.6875 - custom__recall: 0.5500      \n",
      "Epoch 59: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.3952 - accuracy: 0.6500 - custom__precision: 0.6875 - custom__recall: 0.5500 - val_loss: 0.3983 - val_accuracy: 0.5000 - val_custom__precision: 0.5000 - val_custom__recall: 0.1250\n",
      "Epoch 60/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3973 - accuracy: 0.5375 - custom__precision: 0.5556 - custom__recall: 0.3750\n",
      "Epoch 60: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.3973 - accuracy: 0.5375 - custom__precision: 0.5556 - custom__recall: 0.3750 - val_loss: 0.3096 - val_accuracy: 0.6250 - val_custom__precision: 0.7500 - val_custom__recall: 0.3750\n",
      "Epoch 61/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5952 - accuracy: 0.5375 - custom__precision: 0.5455 - custom__recall: 0.4500\n",
      "Epoch 61: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.5952 - accuracy: 0.5375 - custom__precision: 0.5455 - custom__recall: 0.4500 - val_loss: 0.6901 - val_accuracy: 0.5000 - val_custom__precision: 0.5000 - val_custom__recall: 0.2500\n",
      "Epoch 62/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4429 - accuracy: 0.5250 - custom__precision: 0.5312 - custom__recall: 0.4250      \n",
      "Epoch 62: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 0.4429 - accuracy: 0.5250 - custom__precision: 0.5312 - custom__recall: 0.4250 - val_loss: 0.3528 - val_accuracy: 0.5000 - val_custom__precision: 0.5000 - val_custom__recall: 0.2500\n",
      "Epoch 63/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3084 - accuracy: 0.5875 - custom__precision: 0.6000 - custom__recall: 0.5250\n",
      "Epoch 63: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.3084 - accuracy: 0.5875 - custom__precision: 0.6000 - custom__recall: 0.5250 - val_loss: 0.2676 - val_accuracy: 0.5625 - val_custom__precision: 0.6667 - val_custom__recall: 0.2500\n",
      "Epoch 64/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2314 - accuracy: 0.7125 - custom__precision: 0.7576 - custom__recall: 0.6250\n",
      "Epoch 64: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.2314 - accuracy: 0.7125 - custom__precision: 0.7576 - custom__recall: 0.6250 - val_loss: 0.2588 - val_accuracy: 0.5625 - val_custom__precision: 0.6667 - val_custom__recall: 0.2500\n",
      "Epoch 65/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2393 - accuracy: 0.5875 - custom__precision: 0.5814 - custom__recall: 0.6250\n",
      "Epoch 65: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.2393 - accuracy: 0.5875 - custom__precision: 0.5814 - custom__recall: 0.6250 - val_loss: 0.2376 - val_accuracy: 0.5000 - val_custom__precision: 0.5000 - val_custom__recall: 0.2500\n",
      "Epoch 66/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.2069 - accuracy: 0.7222 - custom__precision: 0.6889 - custom__recall: 0.8378\n",
      "Epoch 66: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.1936 - accuracy: 0.7375 - custom__precision: 0.7021 - custom__recall: 0.8250 - val_loss: 0.2229 - val_accuracy: 0.5000 - val_custom__precision: 0.5000 - val_custom__recall: 0.2500\n",
      "Epoch 67/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.2088 - accuracy: 0.6389 - custom__precision: 0.6047 - custom__recall: 0.7429\n",
      "Epoch 67: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.2109 - accuracy: 0.6250 - custom__precision: 0.6087 - custom__recall: 0.7000 - val_loss: 0.2663 - val_accuracy: 0.5000 - val_custom__precision: 0.5000 - val_custom__recall: 0.2500\n",
      "Epoch 68/200\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.2037 - accuracy: 0.6562 - custom__precision: 0.6316 - custom__recall: 0.7500\n",
      "Epoch 68: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.1972 - accuracy: 0.6625 - custom__precision: 0.6327 - custom__recall: 0.7750 - val_loss: 0.2086 - val_accuracy: 0.5625 - val_custom__precision: 0.6000 - val_custom__recall: 0.3750\n",
      "Epoch 69/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1873 - accuracy: 0.6875 - custom__precision: 0.6923 - custom__recall: 0.6750\n",
      "Epoch 69: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 2s 188ms/step - loss: 0.1873 - accuracy: 0.6875 - custom__precision: 0.6923 - custom__recall: 0.6750 - val_loss: 0.2256 - val_accuracy: 0.5000 - val_custom__precision: 0.5000 - val_custom__recall: 0.2500\n",
      "Epoch 70/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2012 - accuracy: 0.7625 - custom__precision: 0.7333 - custom__recall: 0.8250      \n",
      "Epoch 70: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.2012 - accuracy: 0.7625 - custom__precision: 0.7333 - custom__recall: 0.8250 - val_loss: 0.2267 - val_accuracy: 0.6875 - val_custom__precision: 0.8000 - val_custom__recall: 0.5000\n",
      "Epoch 71/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2197 - accuracy: 0.6500 - custom__precision: 0.6429 - custom__recall: 0.6750\n",
      "Epoch 71: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.2197 - accuracy: 0.6500 - custom__precision: 0.6429 - custom__recall: 0.6750 - val_loss: 0.5427 - val_accuracy: 0.5000 - val_custom__precision: 0.5000 - val_custom__recall: 0.2500\n",
      "Epoch 72/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3524 - accuracy: 0.6500 - custom__precision: 0.6667 - custom__recall: 0.6000\n",
      "Epoch 72: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3524 - accuracy: 0.6500 - custom__precision: 0.6667 - custom__recall: 0.6000 - val_loss: 0.3227 - val_accuracy: 0.6250 - val_custom__precision: 1.0000 - val_custom__recall: 0.2500\n",
      "Epoch 73/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.3049 - accuracy: 0.6250 - custom__precision: 0.6053 - custom__recall: 0.6571\n",
      "Epoch 73: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 0.3123 - accuracy: 0.6000 - custom__precision: 0.6000 - custom__recall: 0.6000 - val_loss: 0.3188 - val_accuracy: 0.5625 - val_custom__precision: 1.0000 - val_custom__recall: 0.1250\n",
      "Epoch 74/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2464 - accuracy: 0.6625 - custom__precision: 0.6512 - custom__recall: 0.7000\n",
      "Epoch 74: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.2464 - accuracy: 0.6625 - custom__precision: 0.6512 - custom__recall: 0.7000 - val_loss: 0.2892 - val_accuracy: 0.5625 - val_custom__precision: 1.0000 - val_custom__recall: 0.1250\n",
      "Epoch 75/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.2230 - accuracy: 0.6806 - custom__precision: 0.6667 - custom__recall: 0.7222\n",
      "Epoch 75: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 0.2282 - accuracy: 0.6750 - custom__precision: 0.6522 - custom__recall: 0.7500 - val_loss: 0.2579 - val_accuracy: 0.5625 - val_custom__precision: 0.6000 - val_custom__recall: 0.3750\n",
      "Epoch 76/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2166 - accuracy: 0.6875 - custom__precision: 0.6596 - custom__recall: 0.7750\n",
      "Epoch 76: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.2166 - accuracy: 0.6875 - custom__precision: 0.6596 - custom__recall: 0.7750 - val_loss: 0.2345 - val_accuracy: 0.6250 - val_custom__precision: 1.0000 - val_custom__recall: 0.2500\n",
      "Epoch 77/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2159 - accuracy: 0.7250 - custom__precision: 0.7143 - custom__recall: 0.7500\n",
      "Epoch 77: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.2159 - accuracy: 0.7250 - custom__precision: 0.7143 - custom__recall: 0.7500 - val_loss: 0.2482 - val_accuracy: 0.5625 - val_custom__precision: 0.6000 - val_custom__recall: 0.3750\n",
      "Epoch 78/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2094 - accuracy: 0.6625 - custom__precision: 0.6327 - custom__recall: 0.7750\n",
      "Epoch 78: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.2094 - accuracy: 0.6625 - custom__precision: 0.6327 - custom__recall: 0.7750 - val_loss: 0.2300 - val_accuracy: 0.6250 - val_custom__precision: 0.7500 - val_custom__recall: 0.3750\n",
      "Epoch 79/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1941 - accuracy: 0.6875 - custom__precision: 0.6531 - custom__recall: 0.8000\n",
      "Epoch 79: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.1941 - accuracy: 0.6875 - custom__precision: 0.6531 - custom__recall: 0.8000 - val_loss: 0.1990 - val_accuracy: 0.7500 - val_custom__precision: 0.8333 - val_custom__recall: 0.6250\n",
      "Epoch 80/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1931 - accuracy: 0.7500 - custom__precision: 0.7000 - custom__recall: 0.8750\n",
      "Epoch 80: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.1931 - accuracy: 0.7500 - custom__precision: 0.7000 - custom__recall: 0.8750 - val_loss: 0.2288 - val_accuracy: 0.6875 - val_custom__precision: 1.0000 - val_custom__recall: 0.3750\n",
      "Epoch 81/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2005 - accuracy: 0.7125 - custom__precision: 0.6809 - custom__recall: 0.8000\n",
      "Epoch 81: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.2005 - accuracy: 0.7125 - custom__precision: 0.6809 - custom__recall: 0.8000 - val_loss: 0.2137 - val_accuracy: 0.8125 - val_custom__precision: 1.0000 - val_custom__recall: 0.6250\n",
      "Epoch 82/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.1903 - accuracy: 0.7361 - custom__precision: 0.7000 - custom__recall: 0.8000\n",
      "Epoch 82: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.1807 - accuracy: 0.7500 - custom__precision: 0.7273 - custom__recall: 0.8000 - val_loss: 0.2028 - val_accuracy: 0.7500 - val_custom__precision: 1.0000 - val_custom__recall: 0.5000\n",
      "Epoch 83/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2376 - accuracy: 0.6625 - custom__precision: 0.6667 - custom__recall: 0.6500\n",
      "Epoch 83: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.2376 - accuracy: 0.6625 - custom__precision: 0.6667 - custom__recall: 0.6500 - val_loss: 0.2071 - val_accuracy: 0.6875 - val_custom__precision: 0.7143 - val_custom__recall: 0.6250\n",
      "Epoch 84/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1982 - accuracy: 0.6625 - custom__precision: 0.6512 - custom__recall: 0.7000\n",
      "Epoch 84: val_loss did not improve from 0.19173\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 0.1982 - accuracy: 0.6625 - custom__precision: 0.6512 - custom__recall: 0.7000 - val_loss: 0.2181 - val_accuracy: 0.6875 - val_custom__precision: 0.6667 - val_custom__recall: 0.7500\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "model_file = \"weights/best_fit.hdf5\"\n",
    "log_dir = \"logs_new\"\n",
    "\n",
    "siamese_model.compile(loss=contrastive_loss, \n",
    "                      optimizer=tf.keras.optimizers.Adam(),\n",
    "                      metrics=[Custom_Accuracy(), Custom_Precision(), Custom_Recall()]\n",
    "                      )\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 30, restore_best_weights = False)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(model_file, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\n",
    "\n",
    "history = siamese_model.fit(train_dataset, \n",
    "                            steps_per_epoch=math.ceil(len(training_pairs) / BATCH_SIZE_TRAIN), \n",
    "                            validation_data=validation_dataset, \n",
    "                            epochs = EPOCHS,\n",
    "                            callbacks=[early_stop, checkpoint, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 560ms/step\n",
      "Distance: [0.24474418]\n",
      "Predicted: [ True]\n",
      "Label: True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Distance: [1.1463075]\n",
      "Predicted: [False]\n",
      "Label: False\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Distance: [0.3402268]\n",
      "Predicted: [ True]\n",
      "Label: True\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Distance: [0.41289222]\n",
      "Predicted: [ True]\n",
      "Label: False\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Distance: [0.4990391]\n",
      "Predicted: [ True]\n",
      "Label: True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Distance: [0.40283433]\n",
      "Predicted: [ True]\n",
      "Label: False\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Distance: [0.7710911]\n",
      "Predicted: [False]\n",
      "Label: True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Distance: [0.7408082]\n",
      "Predicted: [False]\n",
      "Label: False\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Distance: [0.7195218]\n",
      "Predicted: [False]\n",
      "Label: True\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Distance: [0.6582855]\n",
      "Predicted: [False]\n",
      "Label: False\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Distance: [0.49977866]\n",
      "Predicted: [ True]\n",
      "Label: True\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Distance: [0.7361592]\n",
      "Predicted: [False]\n",
      "Label: False\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Distance: [0.3336273]\n",
      "Predicted: [ True]\n",
      "Label: True\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Distance: [0.4695412]\n",
      "Predicted: [ True]\n",
      "Label: False\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Distance: [0.38796377]\n",
      "Predicted: [ True]\n",
      "Label: True\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Distance: [0.55119675]\n",
      "Predicted: [False]\n",
      "Label: False\n"
     ]
    }
   ],
   "source": [
    "for index, pair in enumerate(validation_pairs):\n",
    "    imgA, imgB = preprocess_pair(pair)\n",
    "\n",
    "    # Add batch dimension\n",
    "    imgA = tf.expand_dims(imgA, axis=0)  # (1, 224, 224, 3)\n",
    "    imgB = tf.expand_dims(imgB, axis=0)  # (1, 224, 224, 3)\n",
    "\n",
    "    prediction = siamese_model.predict([imgA, imgB])  \n",
    "    print(f\"Distance: {prediction[0]}\")\n",
    "    print(f\"Predicted: {prediction[0] <= 0.5}\")\n",
    "    print(f\"Label: {bool(validation_pairs_labels[index])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
